{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNR Data Filtration\n",
    "\n",
    "The block of code below is used to clean and filter the raw DNR reports for water quality data of various Wisconsin lakes. The script loops through all the raw data files, extracts the lake name, the county the lake is in, the WBIC code (used later to identify the lake), and the \"lake index\" which is just the loop location. The script removes all the lines in the original CSV except those that contain the secchi depth, chlorophyll-a, and total phosphorus measurements for the date. These values are then saved to a CSV and placed in a folder identified by the lake index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 871/871 [00:59<00:00, 14.63it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Make list of all reports\n",
    "files = glob.glob(\"original_lake_reports/repor*.csv\")\n",
    "\n",
    "# Define an empty dataframe that will store general information for each lake\n",
    "lake_df = pd.DataFrame(columns=[\"lakename\",\"county\", \"WBIC\", \"lake_index\"])\n",
    "\n",
    "# Loop through all files\n",
    "for i in tqdm(range(len(files))):\n",
    "    # set index of lake\n",
    "    index = i\n",
    "    \n",
    "    # Read in report csv\n",
    "    report_df = pd.read_csv(f\"{files[i]}\", skiprows=8,error_bad_lines=False, index_col=False,usecols=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14])\n",
    "    # Rename Columns so they're easier to work with\n",
    "    report_df = report_df.rename(columns={\" Start Date\":\"date\",\"Secchi (Meters)\":\"secchi_m\", \"Chlorophyll(ug/l)\":\"chla\",\"Total Phosphorus(ug/l)\":\"TP\"})\n",
    "    \n",
    "    # Remove any columns that don't have either a TP or chla value\n",
    "    report_df = report_df.loc[(report_df.TP.notna()  | report_df.chla.notna() | report_df.secchi_m.notna())][[\"date\", \"secchi_m\", \"chla\", \"TP\", \"Perception\"]]\n",
    "    report_df = report_df.reset_index(drop=True)\n",
    "    # Remove the bottom of the dataframe that is now measuring DO and reset index\n",
    "    cut_index = report_df.loc[report_df.date == \"Depth\"].index[0]\n",
    "    report_df = report_df[:cut_index]\n",
    "    \n",
    "    \n",
    "    # Make perception column equal to integer value\n",
    "    perception_series = report_df.Perception.notna()\n",
    "    for j in range(len(report_df)):\n",
    "        if perception_series[j] == True:\n",
    "            report_df.Perception.iloc[j] = report_df.Perception.iloc[j][0]\n",
    "            if report_df.Perception.iloc[j] == \" \":\n",
    "                report_df.Perception.iloc[j] = np.nan\n",
    "            else:\n",
    "                report_df.Perception.iloc[j] = int(report_df.Perception.iloc[j])   \n",
    "    \n",
    "    # Make date column a manageable time series\n",
    "    report_df[\"date\"] = pd.to_datetime(report_df[\"date\"])\n",
    "    \n",
    "    # define a dataframe which will contain only the header of the lake reports; this is how we extract \n",
    "    # the lake name, county, and WBIC\n",
    "    df_name = pd.read_csv(f\"{files[i]}\", nrows=1,skiprows= 3,error_bad_lines=False)\n",
    "    df_name = df_name.rename(columns={\"Lake Name\":\"lakename\", \"County Name\":\"county\", \"Waterbody ID(WBIC)\":\"WBIC\"})\n",
    "    df_name = df_name[[\"lakename\",\"county\",\"WBIC\"]]\n",
    "    \n",
    "    # Add the new lake to the lake_df dataframe\n",
    "    lake_df = lake_df.append({\"lakename\":df_name.lakename.iloc[0], \"county\":df_name.county.iloc[0], \"WBIC\":df_name.WBIC.iloc[0], \"lake_index\":i},ignore_index=True)\n",
    "    # Make a new folder to save the lake data to\n",
    "    os.mkdir(f\"C:/Users/dlcole3/Documents/UW_Madison/Research/Models/Pmodel/lake_reports_full/Lakes/lake{i}\")\n",
    "    # Save the lake data\n",
    "    report_df.to_csv(f\"lake_reports_full/Lakes/lake{i}/DNR_data.csv\",index=False)\n",
    "    \n",
    "# Save the dataframe that contains the lake index, lake name, county, and WBIC code\n",
    "lake_df.to_csv(\"lake_index_WBIC.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the COMID to the lake list\n",
    "\n",
    "The DNR identifies their lake data by the WBIC code. Unfortunately, this code is not used the NHDPlusV2 dataset which we use for identifying waterbodies. To overcome this, we needed a way to match COMID to WBIC code. We do this by using the Hydro Waterbodies dataset, which has shapefiles identified by WBIC codes. In this next block of code, we match the shapefiles of the Hydro Waterbodies set to the shapefiles of the NHDPlusV2 set. We first test whether the centroid of the Hydro Waterbodies are within the NHDPlusV2 waterbodies. This is a fast test, but because of the shapes of some lakes, the centroid lies outside of the waterbody. Therefore, after testing the centroid, we test whether the Hydro Waterbodies intersect any of the NHDPlusV2 waterbodies, and we make sure that if they do, there is only one waterbody which intersects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 871/871 [01:38<00:00,  8.80it/s]\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Read in NHDPlusV2 waterbodies\n",
    "WILakes = pd.read_pickle(\"../graph_construction/WILakes.df\")\n",
    "\n",
    "# Read in the hydro waterbodies shapefiles and set the crs to match the EPSG code of the NHDPlusV2 dataset\n",
    "hw = gpd.GeoDataFrame.from_file(\"Hydro_Waterbodies/24k_Hydro_Waterbodies_(Open_Water).shp\")\n",
    "hw = hw.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Read in the lake_index CSV\n",
    "lake_index = pd.read_csv(\"lake_index_WBIC.csv\")\n",
    "\n",
    "# Take a slice of the dataframe so that we are only testing the lakes which show up in the lake_index\n",
    "hw_new = hw[hw.WATERBOD_2.isin(lake_index.WBIC.values)]\n",
    "hw_new = hw_new.reset_index(drop=True)\n",
    "\n",
    "# Define a set of geometries for the NHDPlusV2 data\n",
    "polys = WILakes.geometry\n",
    "\n",
    "# Add column to the lake_index dataframe that will contain the given COMID\n",
    "lake_index[\"COMID\"] = 0\n",
    "\n",
    "# Loop through all lakes in the lake_index\n",
    "for i in tqdm(range(len(lake_index))):\n",
    "    # Get the WBIC code of the lake index\n",
    "    WBIC    = lake_index.WBIC.iloc[i]\n",
    "    \n",
    "    # Get the hydro waterbodies slice that contains the lake index WBIC code.\n",
    "    hw_WBIC = hw_new[hw_new.WATERBOD_2 == WBIC]\n",
    "    \n",
    "    # If the WBIC code of the lake_index set has no entry, then we skip that lake and we cannot \n",
    "    # find a COMID that matchs\n",
    "    if len(hw_WBIC) == 0:\n",
    "        continue\n",
    "        \n",
    "    # Otherwise, get the centroid of the Hydro Waterbody that matches the WBIC code of the lake_index set\n",
    "    if len(hw_WBIC) > 0:\n",
    "        point   = hw_WBIC.centroid.iloc[0]\n",
    "    \n",
    "    # Loop through the NHDPlusV2 set of lakes, and see if the centroid is inside an NHDPlusV2 lake\n",
    "    for j in range(len(WILakes)):\n",
    "        \n",
    "        # If the centroid is within the NHDPlusV2 lake, set the COMID to match that lake\n",
    "        if point.within(polys.iloc[j]):\n",
    "            lake_index.COMID.iloc[i] = WILakes.COMID.iloc[j]\n",
    "            break\n",
    "\n",
    "    # If the COMID of the lake_index set has been reset, move on to the next iteration\n",
    "    if lake_index.COMID.iloc[i] != 0:\n",
    "        pass\n",
    "\n",
    "    # Otherwise, the centroid may lie outside of the NHDPlusV2 lake. Instead, test if it intersects. It is possible that\n",
    "    # multiple NHDPlusV2 lakes intersect the same Hydro Waterbody. If this is the case, we will not set a COMID because we \n",
    "    # cannot tell which lake it corresponds to. \n",
    "    else:\n",
    "        comid_val = []\n",
    "        counter = 0\n",
    "        for j in range(len(WILakes)):\n",
    "            if hw_WBIC.geometry.iloc[0].intersects(polys.iloc[j]):\n",
    "                comid_val = np.append(comid_val, WILakes.COMID.iloc[j])\n",
    "                counter += 1\n",
    "\n",
    "        if counter == 1:\n",
    "            lake_index.COMID.iloc[i] = comid_val[0]\n",
    "\n",
    "# Save the new lake_index dataframe that now contains COMIDs\n",
    "lake_index.to_csv(\"lake_index_WBIC_COMID.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
